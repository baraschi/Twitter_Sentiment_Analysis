{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "from data_loading import *\n",
    "from data_cleaning import *\n",
    "from utils import *\n",
    "from embeddings import *\n",
    "from prediction import *\n",
    "import itertools\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "\n",
    "# load training set as DataFrame\n",
    "train = load_df(TRAIN_NEG, TRAIN_POS, TWEET, LABEL, LABEL_NEG, LABEL_POS)\n",
    "test = pd.DataFrame({TWEET: load_txt(TEST_DATA)})\n",
    "# set patterns to remove, replace, and replace with\n",
    "to_remove = \"<user>\"\n",
    "to_replace = \"[^a-zA-Z#]\"\n",
    "replace_value = \" \"\n",
    "# clean training set\n",
    "#train = clean(train, TWEET, CLEAN_TWEET, to_remove, to_replace, replace_value,option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_clean_options():\n",
    "    all_clean_options = list()\n",
    "    \n",
    "    nb_options = 4\n",
    "    bin_combinations = list(itertools.product([0,1], repeat=nb_options))\n",
    "    \n",
    "    for combination in bin_combinations:\n",
    "        all_clean_options.append({\n",
    "            'duplicates': combination[0],\n",
    "            'replace_pattern': combination[1],\n",
    "            'stop_words': combination[2],\n",
    "            'stemming': combination[3]\n",
    "        })\n",
    "\n",
    "    return all_clean_options\n",
    "    \n",
    "def accuracies(classifiers, train, min_df, min_features, max_features, feature_step, ngram_range):\n",
    "    \"\"\" Iterates on the given parameters range and using the specified classifier, logs accuracies obtained into a\n",
    "    Dataframe\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(columns=['method','n-gram', 'replace_pattern','stop_words','steming','nb_features','accuracy'])\n",
    "    \n",
    "    all_clean_options = generate_all_clean_options()\n",
    "    features_range = np.arange(min_features, max_features, step=feature_step)\n",
    "    \n",
    "    nb_steps = len(all_clean_options) * len(ngram_range) * len(features_range) * len(classifiers)\n",
    "    step = 0\n",
    "    \n",
    "    for clean_options in all_clean_options:\n",
    "        log_msg = '\\033[1m{p:.2f}%\\033[0m ({step}/{nb_step}), cleaning data with {clean_options}...'.format(\n",
    "            clean_options= clean_options,\n",
    "            p = 100 * (step / nb_steps),\n",
    "            step = step + 1,\n",
    "            nb_step = nb_steps\n",
    "        )\n",
    "        printOver(log_msg)\n",
    "    \n",
    "        train = clean(train, TWEET, CLEAN_TWEET, to_remove, to_replace, replace_value,clean_options)\n",
    "\n",
    "        for classifier_str in classifiers:\n",
    "            if classifier_str is \"tfidf\":\n",
    "                classifier = classify_tfidf\n",
    "            elif classifier_str is \"bow\":\n",
    "                classifier = classify_bow\n",
    "            else:\n",
    "                raise Exception(\"Unsupported classify method: \"+classifier_str)\n",
    "                \n",
    "            for ngram in ngram_range: \n",
    "                for nb_features in features_range:\n",
    "                    log_msg = '\\033[1m{p:.2f}%\\033[0m ({step}/{nb_step}), classifying with {classifier}, {clean_options}...'.format(\n",
    "                        classifier = classifier_str,\n",
    "                        clean_options= clean_options,\n",
    "                        p = 100 * (step / nb_steps),\n",
    "                        step = step + 1,\n",
    "                        nb_step = nb_steps\n",
    "                    )\n",
    "                    printOver(log_msg)\n",
    "\n",
    "                    accuracy = classifier(train, None, tweets_col = CLEAN_TWEET, ngram_range=ngram, max_features =nb_features, min_df=min_df)\n",
    "                    df = df.append({\n",
    "                        'method': classifier.__name__, \n",
    "                        'n-gram': ngram[1], \n",
    "                        'duplicates': clean_options['duplicates'],\n",
    "                        'replace_pattern': clean_options['replace_pattern'],\n",
    "                        'stop_words': clean_options['stop_words'],\n",
    "                        'stemming': clean_options['stemming'],\n",
    "                        'nb_features': nb_features,\n",
    "                        'accuracy': accuracy\n",
    "                    }, ignore_index=True)\n",
    "                    step = step + 1\n",
    "                    \n",
    "    log_msg = '\\033[1mDone!\\033[0m ({step}/{nb_step})'.format(\n",
    "            p = 100 * (step / nb_steps),\n",
    "            step = step,\n",
    "            nb_step = nb_steps\n",
    "    )\n",
    "    printOver(log_msg)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[1m0.00%\u001b[0m (1/960), cleaning data with {'duplicates': 0, 'replace_pattern': 0, 'stop_words': 0, 'stemming': 0}..."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Before dup: (200000, 3)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m0.31%\u001b[0m (4/960), classifying with bow, {'duplicates': 0, 'replace_pattern': 0, 'stop_words': 0, 'stemming': 0}..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d1619c96ee09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m110000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfeature_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-13-c0e8ab4903bb>\u001b[0m in \u001b[0;36maccuracies\u001b[1;34m(classifiers, train, min_df, min_features, max_features, feature_step, ngram_range)\u001b[0m\n\u001b[0;32m     57\u001b[0m                     \u001b[0mprintOver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCLEAN_TWEET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnb_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                     df = df.append({\n\u001b[0;32m     61\u001b[0m                         \u001b[1;34m'method'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Docs\\GitHub\\ML_Project_2\\prediction.py\u001b[0m in \u001b[0;36mclassify_bow\u001b[1;34m(train, test, tweets_col, filename, max_df, min_df, max_features, ngram_range)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mxtrain_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxvalid_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mlreg_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mlreg_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mprediction_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlreg_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalid_bow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# predicting on the validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_validation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1231\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m                 sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1234\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         epsilon, sample_weight)\n\u001b[0m\u001b[0;32m    891\u001b[0m     \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = accuracies(\n",
    "    [\"bow\", \"tfidf\"],\n",
    "    train = train,\n",
    "    min_df=1, \n",
    "    min_features=10000, \n",
    "    max_features=110000,\n",
    "    feature_step=10000, \n",
    "    ngram_range=[(1,1), (1,2), (1,3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31.25%\u001b[0m (11/32), classifying with bow, {'duplicates': 0, 'replace_pattern': 1, 'stop_words': 0, 'stemming': 1}....."
     ]
    }
   ],
   "source": [
    "df = accuracies(\n",
    "    [\"bow\", \"tfidf\"],\n",
    "    train = train,\n",
    "    min_df=1, \n",
    "    min_features=10000, \n",
    "    max_features=11000,\n",
    "    feature_step=10000, \n",
    "    ngram_range=[(1,1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(accs_clean, accs_noclean, legends, colors, title, axis_labels, filename):\n",
    "    nb_plots = len(accs_clean)\n",
    "    plotable_df_clean = accs_clean.transpose()\n",
    "    plotable_df_clean['nfeatures'] = plotable_df_clean.index\n",
    "\n",
    "    plotable_df_noclean = accs_noclean.transpose()\n",
    "    plotable_df_noclean['nfeatures'] = plotable_df_noclean.index\n",
    "        \n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    for i in range(0, nb_plots):\n",
    "        plt.plot(plotable_df_clean[i].index, plotable_df_clean[i].values,label=legends[i] + ' (clean)',color=colors[i])\n",
    "        plt.plot(plotable_df_noclean[i].index, plotable_df_noclean[i].values,label=legends[i],color=colors[i], linestyle=':')\n",
    "        \n",
    "    plt.title(title)\n",
    "    plt.xlabel(axis_labels[0])\n",
    "    plt.ylabel(axis_labels[1])\n",
    "    plt.legend()\n",
    "    \n",
    "    if not os.path.exists(PLOTS_FOLDER):\n",
    "        os.makedirs(PLOTS_FOLDER)\n",
    "    plt.savefig(PLOTS_FOLDER + filename + '.pdf') #pdf is a better choice than png as it is vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracies(\n",
    "    bow_accs_clean,\n",
    "    bow_accs_noclean,\n",
    "    legends = ['unigram bow vectorizer', 'bigram bow vectorizer', 'trigram bow vectorizer'],\n",
    "    colors = ['royalblue', 'orangered', 'gold'],\n",
    "    title = 'Accuracy BOW N-gram(1~3)',\n",
    "    axis_labels =['Number of features','Validation set accuracy'],\n",
    "    filename = \"plot_bow\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
